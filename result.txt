\n===================\nRunning without --enforce-eager --batch-size 1 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=64, output_len=2, batch_size=1, enforce_eager=False)

=======
BATCH_SIZE=1 PROMPT_LEN=64 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:05:38 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
[36m(autoscaler +22s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
INFO 04-22 04:06:00 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=192341)[0m INFO 04-22 04:06:00 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:06:03 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:06:03 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=192436)[0m INFO 04-22 04:06:03 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:06:06 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=192341)[0m INFO 04-22 04:06:06 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=192939)[0m INFO 04-22 04:06:00 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:06:22 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=192341)[0m INFO 04-22 04:06:22 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=192939)[0m INFO 04-22 04:06:03 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=192939)[0m INFO 04-22 04:06:06 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=192738)[0m INFO 04-22 04:06:25 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=192939)[0m INFO 04-22 04:06:22 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
INFO 04-22 04:06:29 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=192436)[0m INFO 04-22 04:06:32 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=192738)[0m INFO 04-22 04:06:45 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=192540)[0m INFO 04-22 04:06:50 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=192341)[0m INFO 04-22 04:06:52 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=192939)[0m INFO 04-22 04:06:59 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:07:05 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=192436)[0m INFO 04-22 04:07:07 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:07:12 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:07:12 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:07:12 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=192341)[0m INFO 04-22 04:07:12 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=192341)[0m INFO 04-22 04:07:12 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-22 04:07:18 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=192341)[0m INFO 04-22 04:07:18 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=192939)[0m INFO 04-22 04:07:12 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=192939)[0m INFO 04-22 04:07:12 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:07:18 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=192341)[0m INFO 04-22 04:07:18 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.04491877555847168
decode mean: 10.196089744567871 ms
decode p50: 10.375022888183594 ms
decode p95: 10.375022888183594 ms
decode p99: 10.375022888183594 ms
prefill mean: 36.890387535095215 ms
prefill p50: 40.0393009185791 ms
prefill p95: 40.0393009185791 ms
prefill p99: 40.0393009185791 ms
[36m(RayWorkerWrapper pid=192939)[0m INFO 04-22 04:07:18 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=192939)[0m INFO 04-22 04:07:18 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 2 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=64, output_len=2, batch_size=2, enforce_eager=False)

=======
BATCH_SIZE=2 PROMPT_LEN=64 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:07:31 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:07:50 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=194318)[0m INFO 04-22 04:07:50 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:07:53 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:07:53 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=194740)[0m INFO 04-22 04:07:54 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:07:56 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=194318)[0m INFO 04-22 04:07:56 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=194939)[0m INFO 04-22 04:07:50 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:08:12 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=194318)[0m INFO 04-22 04:08:12 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=194640)[0m INFO 04-22 04:07:54 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=194939)[0m INFO 04-22 04:07:56 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=194318)[0m INFO 04-22 04:08:14 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:08:15 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=194318)[0m INFO 04-22 04:08:32 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=194939)[0m INFO 04-22 04:08:12 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=194740)[0m INFO 04-22 04:08:15 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=194438)[0m INFO 04-22 04:08:37 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:08:42 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:08:47 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:08:47 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:08:47 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=194318)[0m INFO 04-22 04:08:47 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=194318)[0m INFO 04-22 04:08:47 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=194839)[0m INFO 04-22 04:08:42 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:08:53 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=194318)[0m INFO 04-22 04:08:53 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=194939)[0m INFO 04-22 04:08:47 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=194939)[0m INFO 04-22 04:08:47 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:08:53 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=194318)[0m INFO 04-22 04:08:53 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.042130231857299805
decode mean: 10.941386222839355 ms
decode p50: 11.155843734741211 ms
decode p95: 11.155843734741211 ms
decode p99: 11.155843734741211 ms
prefill mean: 34.00719165802002 ms
prefill p50: 38.332223892211914 ms
prefill p95: 38.332223892211914 ms
prefill p99: 38.332223892211914 ms
[36m(RayWorkerWrapper pid=194939)[0m INFO 04-22 04:08:53 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=194939)[0m INFO 04-22 04:08:53 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 4 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=64, output_len=2, batch_size=4, enforce_eager=False)

=======
BATCH_SIZE=4 PROMPT_LEN=64 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:09:05 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:09:25 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=196249)[0m INFO 04-22 04:09:25 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:09:28 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:09:28 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=196599)[0m INFO 04-22 04:09:28 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:09:31 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=196249)[0m INFO 04-22 04:09:31 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=196905)[0m INFO 04-22 04:09:25 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:09:48 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=196249)[0m INFO 04-22 04:09:48 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=196905)[0m INFO 04-22 04:09:28 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=196905)[0m INFO 04-22 04:09:31 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
INFO 04-22 04:09:50 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=196249)[0m INFO 04-22 04:09:50 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:10:09 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=196501)[0m INFO 04-22 04:10:09 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=196905)[0m INFO 04-22 04:09:48 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=196702)[0m INFO 04-22 04:09:51 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=196249)[0m INFO 04-22 04:10:15 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=196702)[0m INFO 04-22 04:10:20 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 3x across cluster][0m
INFO 04-22 04:10:25 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:10:26 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:10:26 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=196249)[0m INFO 04-22 04:10:26 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=196249)[0m INFO 04-22 04:10:26 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-22 04:10:32 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=196249)[0m INFO 04-22 04:10:32 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=196905)[0m INFO 04-22 04:10:26 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=196905)[0m INFO 04-22 04:10:26 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:10:32 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=196249)[0m INFO 04-22 04:10:32 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.04594254493713379
decode mean: 11.275291442871094 ms
decode p50: 11.548519134521484 ms
decode p95: 11.548519134521484 ms
decode p99: 11.548519134521484 ms
prefill mean: 36.900997161865234 ms
prefill p50: 41.19443893432617 ms
prefill p95: 41.19443893432617 ms
prefill p99: 41.19443893432617 ms
[36m(RayWorkerWrapper pid=196905)[0m INFO 04-22 04:10:32 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=196905)[0m INFO 04-22 04:10:32 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 8 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=64, output_len=2, batch_size=8, enforce_eager=False)

=======
BATCH_SIZE=8 PROMPT_LEN=64 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:10:44 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:11:03 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=198230)[0m INFO 04-22 04:11:03 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:11:06 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:11:06 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=198230)[0m INFO 04-22 04:11:06 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:11:09 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=198230)[0m INFO 04-22 04:11:09 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=198873)[0m INFO 04-22 04:11:03 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:11:26 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=198230)[0m INFO 04-22 04:11:26 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=198331)[0m INFO 04-22 04:11:06 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=198873)[0m INFO 04-22 04:11:09 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=198331)[0m INFO 04-22 04:11:28 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:11:28 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=198331)[0m INFO 04-22 04:11:47 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=198873)[0m INFO 04-22 04:11:26 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=198873)[0m INFO 04-22 04:11:30 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:11:50 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=198648)[0m INFO 04-22 04:11:53 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:12:01 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:12:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:12:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=198230)[0m INFO 04-22 04:12:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=198230)[0m INFO 04-22 04:12:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=198230)[0m INFO 04-22 04:11:56 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:12:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=198230)[0m INFO 04-22 04:12:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=198873)[0m INFO 04-22 04:12:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=198873)[0m INFO 04-22 04:12:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:12:07 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
[36m(RayWorkerWrapper pid=198230)[0m INFO 04-22 04:12:07 model_runner.py:1058] Graph capturing finished in 6 secs.
e2e takes 0.04884171485900879
decode mean: 11.583685874938965 ms
decode p50: 11.682271957397461 ms
decode p95: 11.682271957397461 ms
decode p99: 11.682271957397461 ms
prefill mean: 39.110422134399414 ms
prefill p50: 43.85876655578613 ms
prefill p95: 43.85876655578613 ms
prefill p99: 43.85876655578613 ms
[36m(RayWorkerWrapper pid=198873)[0m INFO 04-22 04:12:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=198873)[0m INFO 04-22 04:12:07 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 16 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=64, output_len=2, batch_size=16, enforce_eager=False)

=======
BATCH_SIZE=16 PROMPT_LEN=64 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:12:19 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:12:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=200187)[0m INFO 04-22 04:12:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:12:41 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:12:41 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=200408)[0m INFO 04-22 04:12:42 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:12:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=200187)[0m INFO 04-22 04:12:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=200804)[0m INFO 04-22 04:12:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:13:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=200187)[0m INFO 04-22 04:13:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=200804)[0m INFO 04-22 04:12:42 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=200804)[0m INFO 04-22 04:12:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=200187)[0m INFO 04-22 04:13:04 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:13:04 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=200187)[0m INFO 04-22 04:13:22 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=200804)[0m INFO 04-22 04:13:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=200706)[0m INFO 04-22 04:13:05 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=200804)[0m INFO 04-22 04:13:28 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 3x across cluster][0m
INFO 04-22 04:13:31 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=200408)[0m INFO 04-22 04:13:34 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 3x across cluster][0m
INFO 04-22 04:13:39 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:13:40 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:13:40 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=200187)[0m INFO 04-22 04:13:40 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=200187)[0m INFO 04-22 04:13:40 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-22 04:13:45 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=200187)[0m INFO 04-22 04:13:45 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=200804)[0m INFO 04-22 04:13:40 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=200804)[0m INFO 04-22 04:13:40 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:13:45 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
[36m(RayWorkerWrapper pid=200187)[0m INFO 04-22 04:13:45 model_runner.py:1058] Graph capturing finished in 6 secs.
e2e takes 0.06374096870422363
decode mean: 12.292265892028809 ms
decode p50: 12.374639511108398 ms
decode p95: 12.374639511108398 ms
decode p99: 12.374639511108398 ms
prefill mean: 52.86240577697754 ms
prefill p50: 58.69698524475098 ms
prefill p95: 58.69698524475098 ms
prefill p99: 58.69698524475098 ms
[36m(RayWorkerWrapper pid=200804)[0m INFO 04-22 04:13:45 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=200804)[0m INFO 04-22 04:13:45 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 32 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=64, output_len=2, batch_size=32, enforce_eager=False)

=======
BATCH_SIZE=32 PROMPT_LEN=64 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:13:57 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:14:17 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=202115)[0m INFO 04-22 04:14:17 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:14:20 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:14:20 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=202437)[0m INFO 04-22 04:14:20 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:14:22 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=202115)[0m INFO 04-22 04:14:22 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:14:17 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:14:39 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=202115)[0m INFO 04-22 04:14:39 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:14:20 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:14:22 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:14:40 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:14:41 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:14:59 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:14:39 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=202637)[0m INFO 04-22 04:14:41 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:15:03 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=202637)[0m INFO 04-22 04:15:04 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:15:12 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:15:12 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:15:12 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=202115)[0m INFO 04-22 04:15:12 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=202115)[0m INFO 04-22 04:15:12 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=202437)[0m INFO 04-22 04:15:07 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:15:19 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=202115)[0m INFO 04-22 04:15:19 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:15:12 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:15:12 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:15:19 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=202115)[0m INFO 04-22 04:15:19 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.10001707077026367
decode mean: 13.806462287902832 ms
decode p50: 13.91911506652832 ms
decode p95: 13.91911506652832 ms
decode p99: 13.91911506652832 ms
prefill mean: 83.50861072540283 ms
prefill p50: 88.44923973083496 ms
prefill p95: 88.44923973083496 ms
prefill p99: 88.44923973083496 ms
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:15:19 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=202735)[0m INFO 04-22 04:15:19 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 1 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=128, output_len=2, batch_size=1, enforce_eager=False)

=======
BATCH_SIZE=1 PROMPT_LEN=128 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:15:31 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:15:51 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=204050)[0m INFO 04-22 04:15:51 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:15:54 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:15:54 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=204374)[0m INFO 04-22 04:15:54 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:15:56 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=204050)[0m INFO 04-22 04:15:56 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=204671)[0m INFO 04-22 04:15:51 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:16:12 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=204050)[0m INFO 04-22 04:16:12 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=204671)[0m INFO 04-22 04:15:54 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=204671)[0m INFO 04-22 04:15:56 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
INFO 04-22 04:16:14 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=204568)[0m INFO 04-22 04:16:14 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:16:32 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=204166)[0m INFO 04-22 04:16:35 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=204671)[0m INFO 04-22 04:16:12 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=204471)[0m INFO 04-22 04:16:15 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=204471)[0m INFO 04-22 04:16:40 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:16:49 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:16:49 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:16:49 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=204050)[0m INFO 04-22 04:16:49 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=204050)[0m INFO 04-22 04:16:49 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=204568)[0m INFO 04-22 04:16:44 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:16:55 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=204050)[0m INFO 04-22 04:16:55 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=204671)[0m INFO 04-22 04:16:49 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=204671)[0m INFO 04-22 04:16:49 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:16:55 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=204050)[0m INFO 04-22 04:16:55 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.041357994079589844
decode mean: 10.589003562927246 ms
decode p50: 11.125802993774414 ms
decode p95: 11.125802993774414 ms
decode p99: 11.125802993774414 ms
prefill mean: 34.934043884277344 ms
prefill p50: 40.01331329345703 ms
prefill p95: 40.01331329345703 ms
prefill p99: 40.01331329345703 ms
[36m(RayWorkerWrapper pid=204671)[0m INFO 04-22 04:16:55 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=204671)[0m INFO 04-22 04:16:55 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 2 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=128, output_len=2, batch_size=2, enforce_eager=False)

=======
BATCH_SIZE=2 PROMPT_LEN=128 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:17:08 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:17:27 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=206035)[0m INFO 04-22 04:17:27 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:17:30 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:17:30 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=206656)[0m INFO 04-22 04:17:30 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:17:32 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=206035)[0m INFO 04-22 04:17:32 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=206656)[0m INFO 04-22 04:17:27 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:17:49 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=206035)[0m INFO 04-22 04:17:49 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=206560)[0m INFO 04-22 04:17:30 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=206656)[0m INFO 04-22 04:17:32 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=206150)[0m INFO 04-22 04:17:50 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:17:50 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=206035)[0m INFO 04-22 04:18:09 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=206656)[0m INFO 04-22 04:17:49 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=206258)[0m INFO 04-22 04:17:50 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:18:13 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=206258)[0m INFO 04-22 04:18:16 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:18:24 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:18:24 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:18:24 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=206035)[0m INFO 04-22 04:18:24 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=206035)[0m INFO 04-22 04:18:24 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=206656)[0m INFO 04-22 04:18:19 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:18:30 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=206035)[0m INFO 04-22 04:18:30 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=206656)[0m INFO 04-22 04:18:24 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=206656)[0m INFO 04-22 04:18:24 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:18:30 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=206035)[0m INFO 04-22 04:18:30 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.045967817306518555
decode mean: 11.073589324951172 ms
decode p50: 11.123418807983398 ms
decode p95: 11.123418807983398 ms
decode p99: 11.123418807983398 ms
prefill mean: 41.410088539123535 ms
prefill p50: 49.425363540649414 ms
prefill p95: 49.425363540649414 ms
prefill p99: 49.425363540649414 ms
[36m(RayWorkerWrapper pid=206656)[0m INFO 04-22 04:18:30 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=206656)[0m INFO 04-22 04:18:30 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 4 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=128, output_len=2, batch_size=4, enforce_eager=False)

=======
BATCH_SIZE=4 PROMPT_LEN=128 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:18:42 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:19:02 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=207941)[0m INFO 04-22 04:19:02 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:19:05 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:19:05 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=208365)[0m INFO 04-22 04:19:05 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:19:07 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=207941)[0m INFO 04-22 04:19:07 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=208562)[0m INFO 04-22 04:19:02 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:19:24 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=207941)[0m INFO 04-22 04:19:24 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=208562)[0m INFO 04-22 04:19:05 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=208562)[0m INFO 04-22 04:19:07 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=207941)[0m INFO 04-22 04:19:25 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:19:25 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=207941)[0m INFO 04-22 04:19:43 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=208562)[0m INFO 04-22 04:19:24 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=208465)[0m INFO 04-22 04:19:26 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:19:48 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=208265)[0m INFO 04-22 04:19:50 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:19:58 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:19:58 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:19:58 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=207941)[0m INFO 04-22 04:19:58 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=207941)[0m INFO 04-22 04:19:58 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=208163)[0m INFO 04-22 04:19:53 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:20:04 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=207941)[0m INFO 04-22 04:20:04 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=208562)[0m INFO 04-22 04:19:58 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=208562)[0m INFO 04-22 04:19:58 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:20:04 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
[36m(RayWorkerWrapper pid=207941)[0m INFO 04-22 04:20:04 model_runner.py:1058] Graph capturing finished in 6 secs.
e2e takes 0.04404473304748535
decode mean: 11.405467987060547 ms
decode p50: 11.655092239379883 ms
decode p95: 11.655092239379883 ms
decode p99: 11.655092239379883 ms
prefill mean: 36.52298450469971 ms
prefill p50: 42.28830337524414 ms
prefill p95: 42.28830337524414 ms
prefill p99: 42.28830337524414 ms
[36m(RayWorkerWrapper pid=208562)[0m INFO 04-22 04:20:04 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=208562)[0m INFO 04-22 04:20:04 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 8 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=128, output_len=2, batch_size=8, enforce_eager=False)

=======
BATCH_SIZE=8 PROMPT_LEN=128 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:20:16 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:20:36 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=209907)[0m INFO 04-22 04:20:36 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:20:39 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:20:39 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=209907)[0m INFO 04-22 04:20:39 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:20:42 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=209907)[0m INFO 04-22 04:20:42 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=210527)[0m INFO 04-22 04:20:36 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:20:59 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=209907)[0m INFO 04-22 04:20:59 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=210430)[0m INFO 04-22 04:20:40 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=210527)[0m INFO 04-22 04:20:42 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=210327)[0m INFO 04-22 04:21:00 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:21:00 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=210327)[0m INFO 04-22 04:21:19 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=210527)[0m INFO 04-22 04:20:59 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=210230)[0m INFO 04-22 04:21:01 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:21:23 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=210230)[0m INFO 04-22 04:21:25 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 5x across cluster][0m
INFO 04-22 04:21:32 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:21:32 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:21:32 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=209907)[0m INFO 04-22 04:21:32 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=209907)[0m INFO 04-22 04:21:32 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=209907)[0m INFO 04-22 04:21:27 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:21:38 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=209907)[0m INFO 04-22 04:21:38 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=210527)[0m INFO 04-22 04:21:32 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=210527)[0m INFO 04-22 04:21:32 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:21:38 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
[36m(RayWorkerWrapper pid=209907)[0m INFO 04-22 04:21:38 model_runner.py:1058] Graph capturing finished in 6 secs.
e2e takes 0.06079983711242676
decode mean: 11.284589767456055 ms
decode p50: 11.496305465698242 ms
decode p95: 11.496305465698242 ms
decode p99: 11.496305465698242 ms
prefill mean: 53.36737632751465 ms
prefill p50: 60.08267402648926 ms
prefill p95: 60.08267402648926 ms
prefill p99: 60.08267402648926 ms
[36m(RayWorkerWrapper pid=210527)[0m INFO 04-22 04:21:38 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=210527)[0m INFO 04-22 04:21:38 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 16 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=128, output_len=2, batch_size=16, enforce_eager=False)

=======
BATCH_SIZE=16 PROMPT_LEN=128 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:21:50 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:22:09 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=211807)[0m INFO 04-22 04:22:09 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:22:12 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:22:12 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=212325)[0m INFO 04-22 04:22:12 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:22:14 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=211807)[0m INFO 04-22 04:22:14 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=212430)[0m INFO 04-22 04:22:09 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:22:31 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=211807)[0m INFO 04-22 04:22:31 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=212430)[0m INFO 04-22 04:22:12 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=212430)[0m INFO 04-22 04:22:14 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=212127)[0m INFO 04-22 04:22:32 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:22:33 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=212127)[0m INFO 04-22 04:22:50 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=212430)[0m INFO 04-22 04:22:31 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=212027)[0m INFO 04-22 04:22:33 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:22:54 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=212228)[0m INFO 04-22 04:22:56 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 6x across cluster][0m
INFO 04-22 04:23:01 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:23:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:23:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=211807)[0m INFO 04-22 04:23:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=211807)[0m INFO 04-22 04:23:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-22 04:23:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=211807)[0m INFO 04-22 04:23:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=212430)[0m INFO 04-22 04:23:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=212430)[0m INFO 04-22 04:23:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:23:07 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=211807)[0m INFO 04-22 04:23:07 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.08833980560302734
decode mean: 12.229084968566895 ms
decode p50: 12.698650360107422 ms
decode p95: 12.698650360107422 ms
decode p99: 12.698650360107422 ms
prefill mean: 81.72070980072021 ms
prefill p50: 91.58611297607422 ms
prefill p95: 91.58611297607422 ms
prefill p99: 91.58611297607422 ms
[36m(RayWorkerWrapper pid=212430)[0m INFO 04-22 04:23:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=212430)[0m INFO 04-22 04:23:07 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 32 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=128, output_len=2, batch_size=32, enforce_eager=False)

=======
BATCH_SIZE=32 PROMPT_LEN=128 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:23:19 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:23:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=213688)[0m INFO 04-22 04:23:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:23:41 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:23:41 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=214104)[0m INFO 04-22 04:23:41 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:23:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=213688)[0m INFO 04-22 04:23:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=214303)[0m INFO 04-22 04:23:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:24:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=213688)[0m INFO 04-22 04:24:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=214303)[0m INFO 04-22 04:23:41 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=214303)[0m INFO 04-22 04:23:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
INFO 04-22 04:24:02 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=214005)[0m INFO 04-22 04:24:02 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:24:20 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=214005)[0m INFO 04-22 04:24:21 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=214303)[0m INFO 04-22 04:24:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=214303)[0m INFO 04-22 04:24:02 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:24:30 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:24:30 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:24:30 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=213688)[0m INFO 04-22 04:24:30 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=213688)[0m INFO 04-22 04:24:30 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=214104)[0m INFO 04-22 04:24:25 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 6x across cluster][0m
INFO 04-22 04:24:36 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=213688)[0m INFO 04-22 04:24:36 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=214303)[0m INFO 04-22 04:24:30 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=214303)[0m INFO 04-22 04:24:30 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:24:36 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=213688)[0m INFO 04-22 04:24:36 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.17045187950134277
decode mean: 14.249324798583984 ms
decode p50: 14.895200729370117 ms
decode p95: 14.895200729370117 ms
decode p99: 14.895200729370117 ms
prefill mean: 79.78737354278564 ms
prefill p50: 80.37137985229492 ms
prefill p95: 90.62385559082031 ms
prefill p99: 90.62385559082031 ms
[36m(RayWorkerWrapper pid=214303)[0m INFO 04-22 04:24:36 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=214303)[0m INFO 04-22 04:24:36 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 1 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=256, output_len=2, batch_size=1, enforce_eager=False)

=======
BATCH_SIZE=1 PROMPT_LEN=256 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:24:49 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:25:08 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=215601)[0m INFO 04-22 04:25:08 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:25:11 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:25:11 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=215826)[0m INFO 04-22 04:25:11 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:25:13 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=215601)[0m INFO 04-22 04:25:13 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=216220)[0m INFO 04-22 04:25:08 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:25:29 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=215601)[0m INFO 04-22 04:25:29 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=216220)[0m INFO 04-22 04:25:11 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=216220)[0m INFO 04-22 04:25:13 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
INFO 04-22 04:25:31 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=215601)[0m INFO 04-22 04:25:31 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:25:49 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=216123)[0m INFO 04-22 04:25:50 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=216220)[0m INFO 04-22 04:25:29 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=215923)[0m INFO 04-22 04:25:31 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=215601)[0m INFO 04-22 04:25:55 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:26:01 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:26:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:26:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=215601)[0m INFO 04-22 04:26:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=215601)[0m INFO 04-22 04:26:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=215923)[0m INFO 04-22 04:25:56 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:26:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=215601)[0m INFO 04-22 04:26:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=216220)[0m INFO 04-22 04:26:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=216220)[0m INFO 04-22 04:26:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:26:07 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
[36m(RayWorkerWrapper pid=215601)[0m INFO 04-22 04:26:07 model_runner.py:1058] Graph capturing finished in 6 secs.
e2e takes 0.04178214073181152
decode mean: 10.832905769348145 ms
decode p50: 10.962247848510742 ms
decode p95: 10.962247848510742 ms
decode p99: 10.962247848510742 ms
prefill mean: 34.98542308807373 ms
prefill p50: 40.16709327697754 ms
prefill p95: 40.16709327697754 ms
prefill p99: 40.16709327697754 ms
[36m(RayWorkerWrapper pid=216220)[0m INFO 04-22 04:26:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=216220)[0m INFO 04-22 04:26:07 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 2 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=256, output_len=2, batch_size=2, enforce_eager=False)

=======
BATCH_SIZE=2 PROMPT_LEN=256 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:26:19 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:26:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=217487)[0m INFO 04-22 04:26:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:26:41 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:26:41 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=217807)[0m INFO 04-22 04:26:41 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:26:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=217487)[0m INFO 04-22 04:26:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=218105)[0m INFO 04-22 04:26:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:27:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=217487)[0m INFO 04-22 04:27:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=218007)[0m INFO 04-22 04:26:42 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=218105)[0m INFO 04-22 04:26:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=217904)[0m INFO 04-22 04:27:02 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:27:03 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=218007)[0m INFO 04-22 04:27:21 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=218105)[0m INFO 04-22 04:27:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=217487)[0m INFO 04-22 04:27:03 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:27:21 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=217807)[0m INFO 04-22 04:27:27 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 5x across cluster][0m
INFO 04-22 04:27:33 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:27:34 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:27:34 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=217487)[0m INFO 04-22 04:27:34 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=217487)[0m INFO 04-22 04:27:34 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=217603)[0m INFO 04-22 04:27:28 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:27:40 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=217487)[0m INFO 04-22 04:27:40 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=218105)[0m INFO 04-22 04:27:34 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=218105)[0m INFO 04-22 04:27:34 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:27:40 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=217487)[0m INFO 04-22 04:27:40 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.047508955001831055
decode mean: 11.468291282653809 ms
decode p50: 11.609554290771484 ms
decode p95: 11.609554290771484 ms
decode p99: 11.609554290771484 ms
prefill mean: 38.113951683044434 ms
prefill p50: 41.790008544921875 ms
prefill p95: 41.790008544921875 ms
prefill p99: 41.790008544921875 ms
[36m(RayWorkerWrapper pid=218105)[0m INFO 04-22 04:27:40 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=218105)[0m INFO 04-22 04:27:40 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 4 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=256, output_len=2, batch_size=4, enforce_eager=False)

=======
BATCH_SIZE=4 PROMPT_LEN=256 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:27:51 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:28:10 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=219403)[0m INFO 04-22 04:28:10 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:28:13 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:28:13 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=220025)[0m INFO 04-22 04:28:14 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:28:16 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=219403)[0m INFO 04-22 04:28:16 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=220025)[0m INFO 04-22 04:28:10 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:28:32 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=219403)[0m INFO 04-22 04:28:32 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=219923)[0m INFO 04-22 04:28:14 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=220025)[0m INFO 04-22 04:28:16 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
INFO 04-22 04:28:34 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=219726)[0m INFO 04-22 04:28:34 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:28:51 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=219626)[0m INFO 04-22 04:28:53 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=220025)[0m INFO 04-22 04:28:32 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=219626)[0m INFO 04-22 04:28:35 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=219519)[0m INFO 04-22 04:29:00 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 5x across cluster][0m
INFO 04-22 04:29:06 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:29:06 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:29:06 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=219403)[0m INFO 04-22 04:29:06 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=219403)[0m INFO 04-22 04:29:06 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=219823)[0m INFO 04-22 04:29:00 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:29:12 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=219403)[0m INFO 04-22 04:29:12 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=220025)[0m INFO 04-22 04:29:06 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=220025)[0m INFO 04-22 04:29:06 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:29:12 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
[36m(RayWorkerWrapper pid=219403)[0m INFO 04-22 04:29:12 model_runner.py:1058] Graph capturing finished in 6 secs.
e2e takes 0.06146740913391113
decode mean: 12.263655662536621 ms
decode p50: 12.48478889465332 ms
decode p95: 12.48478889465332 ms
decode p99: 12.48478889465332 ms
prefill mean: 53.619384765625 ms
prefill p50: 60.30416488647461 ms
prefill p95: 60.30416488647461 ms
prefill p99: 60.30416488647461 ms
[36m(RayWorkerWrapper pid=220025)[0m INFO 04-22 04:29:12 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=220025)[0m INFO 04-22 04:29:12 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 8 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=256, output_len=2, batch_size=8, enforce_eager=False)

=======
BATCH_SIZE=8 PROMPT_LEN=256 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:29:24 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:29:43 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=221300)[0m INFO 04-22 04:29:43 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:29:46 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:29:46 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=221619)[0m INFO 04-22 04:29:46 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:29:49 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=221300)[0m INFO 04-22 04:29:49 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=221919)[0m INFO 04-22 04:29:43 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:30:06 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=221300)[0m INFO 04-22 04:30:06 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=221919)[0m INFO 04-22 04:29:46 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=221919)[0m INFO 04-22 04:29:49 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=221300)[0m INFO 04-22 04:30:08 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:30:08 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=221300)[0m INFO 04-22 04:30:26 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=221919)[0m INFO 04-22 04:30:06 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=221522)[0m INFO 04-22 04:30:08 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=221919)[0m INFO 04-22 04:30:32 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:30:33 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:30:39 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:30:39 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:30:39 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=221300)[0m INFO 04-22 04:30:39 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=221300)[0m INFO 04-22 04:30:39 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=221522)[0m INFO 04-22 04:30:34 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:30:44 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=221300)[0m INFO 04-22 04:30:44 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=221919)[0m INFO 04-22 04:30:39 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=221919)[0m INFO 04-22 04:30:39 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:30:44 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=221300)[0m INFO 04-22 04:30:44 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.09357452392578125
decode mean: 11.677384376525879 ms
decode p50: 11.87753677368164 ms
decode p95: 11.87753677368164 ms
decode p99: 11.87753677368164 ms
prefill mean: 83.25505256652832 ms
prefill p50: 87.6457691192627 ms
prefill p95: 87.6457691192627 ms
prefill p99: 87.6457691192627 ms
[36m(RayWorkerWrapper pid=221919)[0m INFO 04-22 04:30:44 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=221919)[0m INFO 04-22 04:30:44 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 16 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=256, output_len=2, batch_size=16, enforce_eager=False)

=======
BATCH_SIZE=16 PROMPT_LEN=256 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:30:56 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:31:15 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=223231)[0m INFO 04-22 04:31:15 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:31:19 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:31:19 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=223849)[0m INFO 04-22 04:31:19 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:31:22 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=223231)[0m INFO 04-22 04:31:22 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=223849)[0m INFO 04-22 04:31:15 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:31:38 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=223231)[0m INFO 04-22 04:31:38 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=223749)[0m INFO 04-22 04:31:19 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=223849)[0m INFO 04-22 04:31:22 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=223346)[0m INFO 04-22 04:31:40 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:31:40 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=223346)[0m INFO 04-22 04:31:58 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=223849)[0m INFO 04-22 04:31:38 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=223450)[0m INFO 04-22 04:31:40 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:32:02 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=223450)[0m INFO 04-22 04:32:03 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:32:11 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:32:11 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:32:11 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=223231)[0m INFO 04-22 04:32:11 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=223231)[0m INFO 04-22 04:32:11 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=223652)[0m INFO 04-22 04:32:06 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:32:17 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=223231)[0m INFO 04-22 04:32:17 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=223849)[0m INFO 04-22 04:32:11 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=223849)[0m INFO 04-22 04:32:11 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:32:17 model_runner.py:1058] Graph capturing finished in 7 secs.
[36m(RayWorkerWrapper pid=223231)[0m INFO 04-22 04:32:17 model_runner.py:1058] Graph capturing finished in 7 secs.
warn up
e2e takes 0.17117667198181152
decode mean: 12.739300727844238 ms
decode p50: 12.827873229980469 ms
decode p95: 12.827873229980469 ms
decode p99: 12.827873229980469 ms
prefill mean: 80.00427484512329 ms
prefill p50: 78.98426055908203 ms
prefill p95: 89.07961845397949 ms
prefill p99: 89.07961845397949 ms
[36m(RayWorkerWrapper pid=223849)[0m INFO 04-22 04:32:17 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=223849)[0m INFO 04-22 04:32:17 model_runner.py:1058] Graph capturing finished in 7 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 32 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=256, output_len=2, batch_size=32, enforce_eager=False)

=======
BATCH_SIZE=32 PROMPT_LEN=256 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:32:29 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:32:48 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=225151)[0m INFO 04-22 04:32:48 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:32:52 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:32:52 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=225151)[0m INFO 04-22 04:32:52 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:32:55 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=225151)[0m INFO 04-22 04:32:55 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=225774)[0m INFO 04-22 04:32:48 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:33:11 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=225151)[0m INFO 04-22 04:33:11 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=225774)[0m INFO 04-22 04:32:52 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=225774)[0m INFO 04-22 04:32:55 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
INFO 04-22 04:33:13 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=225376)[0m INFO 04-22 04:33:13 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:33:31 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=225473)[0m INFO 04-22 04:33:32 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=225774)[0m INFO 04-22 04:33:11 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=225673)[0m INFO 04-22 04:33:14 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=225151)[0m INFO 04-22 04:33:38 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 5x across cluster][0m
INFO 04-22 04:33:44 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:33:44 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:33:44 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=225151)[0m INFO 04-22 04:33:44 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=225151)[0m INFO 04-22 04:33:44 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=225267)[0m INFO 04-22 04:33:38 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:33:50 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=225151)[0m INFO 04-22 04:33:50 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=225774)[0m INFO 04-22 04:33:44 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=225774)[0m INFO 04-22 04:33:44 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:33:50 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
[36m(RayWorkerWrapper pid=225151)[0m INFO 04-22 04:33:50 model_runner.py:1058] Graph capturing finished in 6 secs.
e2e takes 0.2998838424682617
decode mean: 13.752579689025879 ms
decode p50: 14.003515243530273 ms
decode p95: 14.003515243530273 ms
decode p99: 14.003515243530273 ms
prefill mean: 74.79891180992126 ms
prefill p50: 77.16512680053711 ms
prefill p95: 88.40751647949219 ms
prefill p99: 88.40751647949219 ms
[36m(RayWorkerWrapper pid=225774)[0m INFO 04-22 04:33:50 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=225774)[0m INFO 04-22 04:33:50 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 1 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=512, output_len=2, batch_size=1, enforce_eager=False)

=======
BATCH_SIZE=1 PROMPT_LEN=512 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:34:02 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:34:21 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=227069)[0m INFO 04-22 04:34:21 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:34:24 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:34:24 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=227393)[0m INFO 04-22 04:34:24 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:34:26 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=227069)[0m INFO 04-22 04:34:26 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=227688)[0m INFO 04-22 04:34:21 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:34:43 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=227069)[0m INFO 04-22 04:34:43 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=227688)[0m INFO 04-22 04:34:24 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=227688)[0m INFO 04-22 04:34:26 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=227069)[0m INFO 04-22 04:34:45 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:34:45 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=227187)[0m INFO 04-22 04:35:03 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=227688)[0m INFO 04-22 04:34:43 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=227393)[0m INFO 04-22 04:34:46 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=227688)[0m INFO 04-22 04:35:09 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:35:10 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:35:17 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:35:17 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:35:17 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=227069)[0m INFO 04-22 04:35:17 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=227069)[0m INFO 04-22 04:35:17 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=227290)[0m INFO 04-22 04:35:12 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:35:23 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=227069)[0m INFO 04-22 04:35:23 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=227688)[0m INFO 04-22 04:35:17 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=227688)[0m INFO 04-22 04:35:17 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:35:24 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=227187)[0m INFO 04-22 04:35:24 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.045389652252197266
decode mean: 11.40296459197998 ms
decode p50: 11.433124542236328 ms
decode p95: 11.433124542236328 ms
decode p99: 11.433124542236328 ms
prefill mean: 37.425994873046875 ms
prefill p50: 42.25564002990723 ms
prefill p95: 42.25564002990723 ms
prefill p99: 42.25564002990723 ms
[36m(RayWorkerWrapper pid=227688)[0m INFO 04-22 04:35:23 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=227069)[0m INFO 04-22 04:35:24 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 2 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=512, output_len=2, batch_size=2, enforce_eager=False)

=======
BATCH_SIZE=2 PROMPT_LEN=512 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:35:35 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:35:55 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=229029)[0m INFO 04-22 04:35:55 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:35:58 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:35:58 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=229650)[0m INFO 04-22 04:35:58 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:36:00 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=229029)[0m INFO 04-22 04:36:00 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=229650)[0m INFO 04-22 04:35:55 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:36:17 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=229029)[0m INFO 04-22 04:36:17 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=229448)[0m INFO 04-22 04:35:58 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=229650)[0m INFO 04-22 04:36:00 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=229545)[0m INFO 04-22 04:36:18 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:36:18 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=229545)[0m INFO 04-22 04:36:36 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=229650)[0m INFO 04-22 04:36:17 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=229448)[0m INFO 04-22 04:36:19 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=229348)[0m INFO 04-22 04:36:43 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 5x across cluster][0m
INFO 04-22 04:36:44 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:36:51 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:36:51 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:36:51 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=229029)[0m INFO 04-22 04:36:51 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=229029)[0m INFO 04-22 04:36:51 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=229141)[0m INFO 04-22 04:36:46 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:36:57 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=229029)[0m INFO 04-22 04:36:57 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=229650)[0m INFO 04-22 04:36:51 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=229650)[0m INFO 04-22 04:36:51 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:36:57 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=229248)[0m INFO 04-22 04:36:57 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.06138253211975098
decode mean: 11.826276779174805 ms
decode p50: 11.858463287353516 ms
decode p95: 11.858463287353516 ms
decode p99: 11.858463287353516 ms
prefill mean: 54.7330379486084 ms
prefill p50: 61.85269355773926 ms
prefill p95: 61.85269355773926 ms
prefill p99: 61.85269355773926 ms
[36m(RayWorkerWrapper pid=229650)[0m INFO 04-22 04:36:57 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=229650)[0m INFO 04-22 04:36:57 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 4 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=512, output_len=2, batch_size=4, enforce_eager=False)

=======
BATCH_SIZE=4 PROMPT_LEN=512 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:37:09 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:37:28 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=230955)[0m INFO 04-22 04:37:28 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:37:32 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:37:32 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=231179)[0m INFO 04-22 04:37:32 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:37:34 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=230955)[0m INFO 04-22 04:37:34 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=231577)[0m INFO 04-22 04:37:28 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:37:51 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=230955)[0m INFO 04-22 04:37:51 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=231577)[0m INFO 04-22 04:37:32 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=231577)[0m INFO 04-22 04:37:34 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=231376)[0m INFO 04-22 04:37:52 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:37:52 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=231376)[0m INFO 04-22 04:38:10 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=231577)[0m INFO 04-22 04:37:51 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=231179)[0m INFO 04-22 04:37:53 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:38:13 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=231179)[0m INFO 04-22 04:38:16 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:38:24 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:38:24 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:38:24 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=230955)[0m INFO 04-22 04:38:24 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=230955)[0m INFO 04-22 04:38:24 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=231577)[0m INFO 04-22 04:38:19 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:38:30 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=230955)[0m INFO 04-22 04:38:30 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=231577)[0m INFO 04-22 04:38:24 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=231577)[0m INFO 04-22 04:38:24 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:38:30 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=230955)[0m INFO 04-22 04:38:30 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.09247708320617676
decode mean: 11.561870574951172 ms
decode p50: 12.224435806274414 ms
decode p95: 12.224435806274414 ms
decode p99: 12.224435806274414 ms
prefill mean: 84.94102954864502 ms
prefill p50: 90.81411361694336 ms
prefill p95: 90.81411361694336 ms
prefill p99: 90.81411361694336 ms
[36m(RayWorkerWrapper pid=231577)[0m INFO 04-22 04:38:30 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=231577)[0m INFO 04-22 04:38:30 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 8 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=512, output_len=2, batch_size=8, enforce_eager=False)

=======
BATCH_SIZE=8 PROMPT_LEN=512 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:38:42 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:39:01 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=232853)[0m INFO 04-22 04:39:01 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:39:05 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:39:05 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=233378)[0m INFO 04-22 04:39:05 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:39:08 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=232853)[0m INFO 04-22 04:39:08 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=233475)[0m INFO 04-22 04:39:01 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:39:24 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=232853)[0m INFO 04-22 04:39:24 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=233475)[0m INFO 04-22 04:39:05 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=233475)[0m INFO 04-22 04:39:08 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=232969)[0m INFO 04-22 04:39:26 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:39:26 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=232969)[0m INFO 04-22 04:39:43 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=233475)[0m INFO 04-22 04:39:24 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=233076)[0m INFO 04-22 04:39:27 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:39:44 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=233274)[0m INFO 04-22 04:39:48 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 3x across cluster][0m
INFO 04-22 04:39:57 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:39:57 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:39:57 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=232853)[0m INFO 04-22 04:39:57 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=232853)[0m INFO 04-22 04:39:57 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=233177)[0m INFO 04-22 04:39:51 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 3x across cluster][0m
INFO 04-22 04:40:03 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=232853)[0m INFO 04-22 04:40:03 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=233475)[0m INFO 04-22 04:39:57 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=233475)[0m INFO 04-22 04:39:57 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:40:03 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=232853)[0m INFO 04-22 04:40:03 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.15553641319274902
decode mean: 11.772632598876953 ms
decode p50: 11.800289154052734 ms
decode p95: 11.800289154052734 ms
decode p99: 11.800289154052734 ms
prefill mean: 81.01731538772583 ms
prefill p50: 78.74703407287598 ms
prefill p95: 105.91864585876465 ms
prefill p99: 105.91864585876465 ms
[36m(RayWorkerWrapper pid=233475)[0m INFO 04-22 04:40:03 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=233475)[0m INFO 04-22 04:40:03 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 16 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=512, output_len=2, batch_size=16, enforce_eager=False)

=======
BATCH_SIZE=16 PROMPT_LEN=512 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:40:16 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:40:35 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=234785)[0m INFO 04-22 04:40:35 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:40:38 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:40:38 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=234884)[0m INFO 04-22 04:40:38 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:40:41 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=234785)[0m INFO 04-22 04:40:41 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=235412)[0m INFO 04-22 04:40:35 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:40:57 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=234785)[0m INFO 04-22 04:40:57 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=235412)[0m INFO 04-22 04:40:38 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=235412)[0m INFO 04-22 04:40:41 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=235212)[0m INFO 04-22 04:40:59 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:40:59 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=235212)[0m INFO 04-22 04:41:18 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=235412)[0m INFO 04-22 04:40:57 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=235006)[0m INFO 04-22 04:41:00 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:41:21 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:41:27 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:41:27 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:41:27 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=234785)[0m INFO 04-22 04:41:27 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=234785)[0m INFO 04-22 04:41:27 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=235006)[0m INFO 04-22 04:41:22 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 6x across cluster][0m
INFO 04-22 04:41:33 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=234785)[0m INFO 04-22 04:41:33 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=235412)[0m INFO 04-22 04:41:27 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=235412)[0m INFO 04-22 04:41:27 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:41:33 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=234785)[0m INFO 04-22 04:41:33 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.29877233505249023
decode mean: 13.213872909545898 ms
decode p50: 13.456106185913086 ms
decode p95: 13.456106185913086 ms
decode p99: 13.456106185913086 ms
prefill mean: 75.09827613830566 ms
prefill p50: 69.73838806152344 ms
prefill p95: 97.98336029052734 ms
prefill p99: 97.98336029052734 ms
[36m(RayWorkerWrapper pid=235109)[0m INFO 04-22 04:41:33 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=235412)[0m INFO 04-22 04:41:33 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 32 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=512, output_len=2, batch_size=32, enforce_eager=False)

=======
BATCH_SIZE=32 PROMPT_LEN=512 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:41:45 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:42:04 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=236695)[0m INFO 04-22 04:42:04 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:42:08 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:42:08 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=236919)[0m INFO 04-22 04:42:08 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:42:11 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=236695)[0m INFO 04-22 04:42:11 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=237317)[0m INFO 04-22 04:42:04 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:42:27 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=236695)[0m INFO 04-22 04:42:27 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=236810)[0m INFO 04-22 04:42:08 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=237317)[0m INFO 04-22 04:42:11 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=236695)[0m INFO 04-22 04:42:29 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:42:29 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=236695)[0m INFO 04-22 04:42:47 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=237317)[0m INFO 04-22 04:42:27 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=237216)[0m INFO 04-22 04:42:30 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
INFO 04-22 04:42:50 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=237216)[0m INFO 04-22 04:42:52 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 3x across cluster][0m
INFO 04-22 04:43:01 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:43:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:43:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=236695)[0m INFO 04-22 04:43:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=236695)[0m INFO 04-22 04:43:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=236919)[0m INFO 04-22 04:42:56 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 3x across cluster][0m
INFO 04-22 04:43:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=236695)[0m INFO 04-22 04:43:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=237317)[0m INFO 04-22 04:43:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=237317)[0m INFO 04-22 04:43:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:43:07 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=236695)[0m INFO 04-22 04:43:07 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.7251956462860107
decode mean: 15.200138092041016 ms
decode p50: 15.203714370727539 ms
decode p95: 15.203714370727539 ms
decode p99: 15.203714370727539 ms
prefill mean: 80.83461225032806 ms
prefill p50: 69.19980049133301 ms
prefill p95: 210.45422554016113 ms
prefill p99: 210.45422554016113 ms
[36m(RayWorkerWrapper pid=237317)[0m INFO 04-22 04:43:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=237317)[0m INFO 04-22 04:43:07 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 1 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=1024, output_len=2, batch_size=1, enforce_eager=False)

=======
BATCH_SIZE=1 PROMPT_LEN=1024 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:43:20 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:43:39 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=238628)[0m INFO 04-22 04:43:39 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:43:43 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:43:43 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=238628)[0m INFO 04-22 04:43:43 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:43:46 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=238628)[0m INFO 04-22 04:43:46 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=239250)[0m INFO 04-22 04:43:39 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:44:02 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=238628)[0m INFO 04-22 04:44:02 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=239250)[0m INFO 04-22 04:43:43 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=239250)[0m INFO 04-22 04:43:46 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=238740)[0m INFO 04-22 04:44:03 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:44:04 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:44:21 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=238740)[0m INFO 04-22 04:44:22 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=239250)[0m INFO 04-22 04:44:02 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=238849)[0m INFO 04-22 04:44:04 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=239146)[0m INFO 04-22 04:44:28 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 5x across cluster][0m
INFO 04-22 04:44:33 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:44:34 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:44:34 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=238628)[0m INFO 04-22 04:44:34 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=238628)[0m INFO 04-22 04:44:34 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=238628)[0m INFO 04-22 04:44:28 model_runner.py:174] Loading model weights took 3.0906 GB
INFO 04-22 04:44:40 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=238628)[0m INFO 04-22 04:44:40 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=239250)[0m INFO 04-22 04:44:34 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=239250)[0m INFO 04-22 04:44:34 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:44:40 model_runner.py:1058] Graph capturing finished in 7 secs.
[36m(RayWorkerWrapper pid=238628)[0m INFO 04-22 04:44:40 model_runner.py:1058] Graph capturing finished in 7 secs.
warn up
e2e takes 0.05987048149108887
decode mean: 11.100292205810547 ms
decode p50: 11.394739151000977 ms
decode p95: 11.394739151000977 ms
decode p99: 11.394739151000977 ms
prefill mean: 53.53212356567383 ms
prefill p50: 59.569358825683594 ms
prefill p95: 59.569358825683594 ms
prefill p99: 59.569358825683594 ms
[36m(RayWorkerWrapper pid=239250)[0m INFO 04-22 04:44:40 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=239250)[0m INFO 04-22 04:44:40 model_runner.py:1058] Graph capturing finished in 7 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 2 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=1024, output_len=2, batch_size=2, enforce_eager=False)

=======
BATCH_SIZE=2 PROMPT_LEN=1024 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:44:52 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:45:11 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=240539)[0m INFO 04-22 04:45:11 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:45:14 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:45:14 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=240862)[0m INFO 04-22 04:45:14 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:45:17 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=240539)[0m INFO 04-22 04:45:17 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=241161)[0m INFO 04-22 04:45:11 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:45:34 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=240539)[0m INFO 04-22 04:45:34 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=240762)[0m INFO 04-22 04:45:15 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=241161)[0m INFO 04-22 04:45:17 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py", line 715, in urlopen
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     httplib_response = self._make_request(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py", line 404, in _make_request
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self._validate_conn(conn)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     conn.connect()
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/connection.py", line 419, in connect
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self.sock = ssl_wrap_socket(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     ssl_sock = _ssl_wrap_socket_impl(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/ssl.py", line 501, in wrap_socket
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     return self.sslsocket_class._create(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/ssl.py", line 1074, in _create
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self.do_handshake()
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/ssl.py", line 1343, in do_handshake
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self._sslobj.do_handshake()
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] ConnectionResetError: [Errno 104] Connection reset by peer
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] 
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] During handling of the above exception, another exception occurred:
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] 
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/requests/adapters.py", line 486, in send
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     resp = conn.urlopen(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py", line 799, in urlopen
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     retries = retries.increment(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/util/retry.py", line 550, in increment
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     raise six.reraise(type(error), error, _stacktrace)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/packages/six.py", line 769, in reraise
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     raise value.with_traceback(tb)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py", line 715, in urlopen
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     httplib_response = self._make_request(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py", line 404, in _make_request
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self._validate_conn(conn)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     conn.connect()
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/connection.py", line 419, in connect
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self.sock = ssl_wrap_socket(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     ssl_sock = _ssl_wrap_socket_impl(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/ssl.py", line 501, in wrap_socket
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     return self.sslsocket_class._create(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/ssl.py", line 1074, in _create
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self.do_handshake()
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/ssl.py", line 1343, in do_handshake
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self._sslobj.do_handshake()
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] 
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] During handling of the above exception, another exception occurred:
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] 
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/default/vllm/vllm/worker/worker_base.py", line 131, in execute_method
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/default/vllm/vllm/worker/worker.py", line 117, in load_model
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/default/vllm/vllm/worker/model_runner.py", line 163, in load_model
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self.model = get_model(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/default/vllm/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/default/vllm/vllm/model_executor/model_loader/loader.py", line 225, in load_model
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     self._get_weights_iterator(model_config.model,
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/default/vllm/vllm/model_executor/model_loader/loader.py", line 202, in _get_weights_iterator
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/default/vllm/vllm/model_executor/model_loader/loader.py", line 173, in _prepare_weights
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     hf_folder = download_weights_from_hf(model_name_or_path,
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/default/vllm/vllm/model_executor/model_loader/weight_utils.py", line 177, in download_weights_from_hf
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     file_list = fs.ls(model_name_or_path, detail=False, revision=revision)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/huggingface_hub/hf_file_system.py", line 287, in ls
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     out = self._ls_tree(path, refresh=refresh, revision=revision, **kwargs)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/huggingface_hub/hf_file_system.py", line 378, in _ls_tree
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     for path_info in tree:
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/huggingface_hub/hf_api.py", line 2923, in list_repo_tree
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/huggingface_hub/utils/_pagination.py", line 36, in paginate
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     r = session.get(path, params=params, headers=headers)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/requests/sessions.py", line 602, in get
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     return self.request("GET", url, **kwargs)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/requests/sessions.py", line 589, in request
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     resp = self.send(prep, **send_kwargs)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/requests/sessions.py", line 703, in send
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     r = adapter.send(request, **kwargs)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/huggingface_hub/utils/_http.py", line 68, in send
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     return super().send(request, *args, **kwargs)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]   File "/home/ray/anaconda3/lib/python3.9/site-packages/requests/adapters.py", line 501, in send
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139]     raise ConnectionError(err, request=request)
[36m(RayWorkerWrapper pid=240862)[0m ERROR 04-22 04:45:35 worker_base.py:139] requests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: a935d21e-b404-4cf1-83f6-663fb55d2716)')
[36m(RayWorkerWrapper pid=240539)[0m INFO 04-22 04:45:35 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:45:36 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=240539)[0m INFO 04-22 04:45:54 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=241161)[0m INFO 04-22 04:45:34 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=241161)[0m INFO 04-22 04:45:36 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 5x across cluster][0m
INFO 04-22 04:45:56 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=240657)[0m INFO 04-22 04:46:00 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=240762)[0m INFO 04-22 04:46:00 model_runner.py:174] Loading model weights took 3.0906 GB
\n===================\nRunning without --enforce-eager --batch-size 4 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=1024, output_len=2, batch_size=4, enforce_eager=False)

=======
BATCH_SIZE=4 PROMPT_LEN=1024 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:46:11 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:46:30 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=242359)[0m INFO 04-22 04:46:30 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:46:33 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:46:33 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=242683)[0m INFO 04-22 04:46:34 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:46:36 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=242359)[0m INFO 04-22 04:46:36 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=242980)[0m INFO 04-22 04:46:30 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:46:53 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=242359)[0m INFO 04-22 04:46:53 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=242478)[0m INFO 04-22 04:46:34 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=242980)[0m INFO 04-22 04:46:36 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
INFO 04-22 04:46:54 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=242980)[0m INFO 04-22 04:46:54 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:47:11 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=242581)[0m INFO 04-22 04:47:13 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=242980)[0m INFO 04-22 04:46:53 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=242881)[0m INFO 04-22 04:46:55 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=242478)[0m INFO 04-22 04:47:20 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:47:27 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:47:27 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:47:27 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=242359)[0m INFO 04-22 04:47:27 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=242359)[0m INFO 04-22 04:47:27 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=242683)[0m INFO 04-22 04:47:22 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:47:33 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=242359)[0m INFO 04-22 04:47:33 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=242980)[0m INFO 04-22 04:47:27 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=242980)[0m INFO 04-22 04:47:27 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:47:33 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=242359)[0m INFO 04-22 04:47:33 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.15877580642700195
decode mean: 11.811137199401855 ms
decode p50: 12.659788131713867 ms
decode p95: 12.659788131713867 ms
decode p99: 12.659788131713867 ms
prefill mean: 78.73302698135376 ms
prefill p50: 80.17659187316895 ms
prefill p95: 90.4231071472168 ms
prefill p99: 90.4231071472168 ms
[36m(RayWorkerWrapper pid=242980)[0m INFO 04-22 04:47:33 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=242980)[0m INFO 04-22 04:47:33 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 8 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=1024, output_len=2, batch_size=8, enforce_eager=False)

=======
BATCH_SIZE=8 PROMPT_LEN=1024 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:47:45 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:48:04 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=244266)[0m INFO 04-22 04:48:04 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:48:07 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:48:07 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=244785)[0m INFO 04-22 04:48:08 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:48:10 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=244266)[0m INFO 04-22 04:48:10 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=244888)[0m INFO 04-22 04:48:04 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:48:27 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=244266)[0m INFO 04-22 04:48:27 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=244888)[0m INFO 04-22 04:48:08 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=244888)[0m INFO 04-22 04:48:10 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
INFO 04-22 04:48:29 weight_utils.py:186] Using model weights format ['*.bin']
[36m(RayWorkerWrapper pid=244785)[0m INFO 04-22 04:48:29 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:48:47 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=244378)[0m INFO 04-22 04:48:49 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=244888)[0m INFO 04-22 04:48:27 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=244378)[0m INFO 04-22 04:48:30 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=244266)[0m INFO 04-22 04:48:54 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:49:01 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:49:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:49:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=244266)[0m INFO 04-22 04:49:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=244266)[0m INFO 04-22 04:49:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=244888)[0m INFO 04-22 04:48:56 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:49:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=244266)[0m INFO 04-22 04:49:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=244888)[0m INFO 04-22 04:49:01 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=244888)[0m INFO 04-22 04:49:01 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:49:07 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=244266)[0m INFO 04-22 04:49:07 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.2966158390045166
decode mean: 12.042999267578125 ms
decode p50: 12.402772903442383 ms
decode p95: 12.402772903442383 ms
decode p99: 12.402772903442383 ms
prefill mean: 76.30452513694763 ms
prefill p50: 78.54676246643066 ms
prefill p95: 91.03178977966309 ms
prefill p99: 91.03178977966309 ms
[36m(RayWorkerWrapper pid=244888)[0m INFO 04-22 04:49:07 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=244687)[0m INFO 04-22 04:49:07 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 16 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=1024, output_len=2, batch_size=16, enforce_eager=False)

=======
BATCH_SIZE=16 PROMPT_LEN=1024 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:49:19 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:49:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=246172)[0m INFO 04-22 04:49:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:49:41 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:49:41 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=246592)[0m INFO 04-22 04:49:42 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:49:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=246172)[0m INFO 04-22 04:49:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=246787)[0m INFO 04-22 04:49:38 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:50:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=246172)[0m INFO 04-22 04:50:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=246787)[0m INFO 04-22 04:49:42 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=246787)[0m INFO 04-22 04:49:44 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=246391)[0m INFO 04-22 04:50:03 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:50:03 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:50:22 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=246391)[0m INFO 04-22 04:50:22 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=246787)[0m INFO 04-22 04:50:01 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=246787)[0m INFO 04-22 04:50:03 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=246689)[0m INFO 04-22 04:50:27 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:50:34 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:50:34 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:50:34 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=246172)[0m INFO 04-22 04:50:34 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=246172)[0m INFO 04-22 04:50:34 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=246592)[0m INFO 04-22 04:50:29 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:50:40 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=246172)[0m INFO 04-22 04:50:40 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=246787)[0m INFO 04-22 04:50:34 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=246787)[0m INFO 04-22 04:50:34 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:50:40 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=246172)[0m INFO 04-22 04:50:40 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 0.5859489440917969
decode mean: 13.937950134277344 ms
decode p50: 13.964414596557617 ms
decode p95: 13.964414596557617 ms
decode p99: 13.964414596557617 ms
prefill mean: 71.88493013381958 ms
prefill p50: 70.07646560668945 ms
prefill p95: 89.39218521118164 ms
prefill p99: 89.39218521118164 ms
[36m(RayWorkerWrapper pid=246787)[0m INFO 04-22 04:50:40 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=246787)[0m INFO 04-22 04:50:40 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
\n===================\nRunning without --enforce-eager --batch-size 32 -tp 8 --model facebook/opt-13b\n===================\n
Namespace(model='facebook/opt-13b', tensor_parallel_size=8, input_len=1024, output_len=2, batch_size=32, enforce_eager=False)

=======
BATCH_SIZE=32 PROMPT_LEN=1024 OUTPUT_LEN=2 ENABLE_CUDA=True TP=8
=======

INFO 04-22 04:50:54 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-13b', speculative_config=None, tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 04-22 04:51:13 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=248110)[0m INFO 04-22 04:51:13 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-22 04:51:16 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-22 04:51:16 selector.py:33] Using XFormers backend.
[36m(RayWorkerWrapper pid=248223)[0m INFO 04-22 04:51:16 selector.py:28] Using FlashAttention backend.
INFO 04-22 04:51:19 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=248110)[0m INFO 04-22 04:51:19 pynccl_utils.py:45] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=248726)[0m INFO 04-22 04:51:13 utils.py:569] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 04-22 04:51:36 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=248110)[0m INFO 04-22 04:51:36 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[36m(RayWorkerWrapper pid=248726)[0m INFO 04-22 04:51:16 selector.py:28] Using FlashAttention backend.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=248726)[0m INFO 04-22 04:51:19 pynccl_utils.py:45] vLLM is using nccl==2.18.1[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=248330)[0m INFO 04-22 04:51:37 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:51:37 weight_utils.py:186] Using model weights format ['*.bin']
INFO 04-22 04:51:56 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=248110)[0m INFO 04-22 04:51:56 model_runner.py:174] Loading model weights took 3.0906 GB
[36m(RayWorkerWrapper pid=248726)[0m INFO 04-22 04:51:36 utils.py:129] reading GPU P2P access cache from /home/ray/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=248526)[0m INFO 04-22 04:51:38 weight_utils.py:186] Using model weights format ['*.bin'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=248223)[0m INFO 04-22 04:52:02 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 4x across cluster][0m
INFO 04-22 04:52:10 ray_gpu_executor.py:213] # GPU blocks: 41579, # CPU blocks: 0
INFO 04-22 04:52:10 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-22 04:52:10 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=248110)[0m INFO 04-22 04:52:10 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=248110)[0m INFO 04-22 04:52:10 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=248526)[0m INFO 04-22 04:52:05 model_runner.py:174] Loading model weights took 3.0906 GB[32m [repeated 2x across cluster][0m
INFO 04-22 04:52:16 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=248110)[0m INFO 04-22 04:52:16 custom_all_reduce.py:230] Registering 2754 cuda graph addresses
[36m(RayWorkerWrapper pid=248726)[0m INFO 04-22 04:52:10 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=248726)[0m INFO 04-22 04:52:10 model_runner.py:981] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 04-22 04:52:16 model_runner.py:1058] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=248110)[0m INFO 04-22 04:52:16 model_runner.py:1058] Graph capturing finished in 6 secs.
warn up
e2e takes 1.3604352474212646
decode mean: 17.57943630218506 ms
decode p50: 18.133878707885742 ms
decode p95: 18.133878707885742 ms
decode p99: 18.133878707885742 ms
prefill mean: 71.73503935337067 ms
prefill p50: 70.55234909057617 ms
prefill p95: 80.04975318908691 ms
prefill p99: 88.11521530151367 ms
[36m(RayWorkerWrapper pid=248726)[0m INFO 04-22 04:52:16 custom_all_reduce.py:230] Registering 2754 cuda graph addresses[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=248726)[0m INFO 04-22 04:52:16 model_runner.py:1058] Graph capturing finished in 6 secs.[32m [repeated 6x across cluster][0m
